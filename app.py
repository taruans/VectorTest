"""
Basit bir Flask uygulamasÄ± ile Milvus Lite kullanan bir vektÃ¶r arama Ã¶rneÄŸi.

Bu uygulama, kullanÄ±cÄ±larÄ±n metin belgelerini veya doÄŸrudan yazdÄ±klarÄ±
metinleri yÃ¼kleyebilmelerine ve ardÄ±ndan bu metinler arasÄ±nda arama
yapabilmelerine olanak tanÄ±r. Belgeler, scikitâ€‘learn'in HashingVectorizer
kullanÄ±larak sabit boyutlu vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r ve Milvus Lite
veritabanÄ±nda saklanÄ±r.

Not: Milvus Lite, yerel bir veritabanÄ± dosyasÄ± Ã¼zerinden Ã§alÄ±ÅŸÄ±r. Bu dosya
oluÅŸturulduÄŸunda Milvus arka planda otomatik olarak ayaÄŸa kalkar ve
Flask uygulamasÄ±yla aynÄ± iÅŸlem iÃ§erisinde Ã§alÄ±ÅŸÄ±r. EÄŸer dinamik kÃ¼tÃ¼phane
yollarÄ±yla ilgili bir sorun yaÅŸanÄ±rsa, aÅŸaÄŸÄ±daki kodda yer alan
LD_LIBRARY_PATH ayarÄ± bu durumu dÃ¼zeltmeye yardÄ±mcÄ± olabilir.
"""

import os
import site
import re
import unicodedata
from flask import Flask, render_template, request, redirect
from pymilvus import MilvusClient, DataType
from sentence_transformers import SentenceTransformer
# --- Turkish-aware normalization & light stemming for better keyword overlap ---
TR_MAP = str.maketrans({
    "Ä±": "i", "Ä°": "i", "I": "i",
    "ÅŸ": "s", "Å": "s",
    "ÄŸ": "g", "Ä": "g",
    "Ã¼": "u", "Ãœ": "u",
    "Ã¶": "o", "Ã–": "o",
    "Ã§": "c", "Ã‡": "c",
})

def _normalize_text(s: str) -> str:
    s = (s or "").lower().translate(TR_MAP)
    return s

def _tokenize_words(s: str):
    return re.findall(r"\w+", _normalize_text(s))

# very light stemmer for common Turkish suffixes; heuristic but effective for search
_SUFFIXES = [
    "leri", "lari", "nin", "nÄ±n", "nun", "nÃ¼n",
    "dan", "den", "tan", "ten",
    "lar", "ler",
    "dÄ±r", "dir", "dur", "dÃ¼r",
    "tir", "tÄ±r", "tur", "tÃ¼r",
    "da", "de", "ta", "te",
    "ya", "ye", "na", "ne",
    "yÄ±", "yi", "yu", "yÃ¼",
    "in", "Ä±n", "un", "Ã¼n",
    "a", "e", "i", "Ä±", "u", "Ã¼",
]

def _stem(token: str) -> str:
    t = token
    for suf in sorted(_SUFFIXES, key=len, reverse=True):
        if t.endswith(suf) and len(t) - len(suf) >= 3:
            return t[: -len(suf)]
    return t

def norm_tokens(s: str):
    toks = [_stem(t) for t in _tokenize_words(s)]
    return {t for t in toks if len(t) > 2}

# minimal Turkish synonym/related-term expansion for short queries
SYNONYMS = {
    "doktor": {"doktor", "hekim", "dr"},
    "hekim": {"doktor", "hekim"},
    "disci": {"dis", "hekimi", "dishekimi"},
    "psikiyatr": {"psikiyatr", "psikiyatrist", "psikoloji", "psikolojik", "ruhsagligi", "mental"},
    "psikiyatrist": {"psikiyatr", "psikiyatrist", "psikoloji", "psikolojik", "ruhsagligi", "mental"},
    "cerrah": {"cerrah", "cerrahi", "ameliyat", "operasyon"},
}

def expand_query_terms(q_tokens):
    expanded = set(q_tokens)
    for t in q_tokens:
        expanded |= { _stem(x) for x in SYNONYMS.get(t, set()) }
    return expanded


# -----------------------------------------------------------------------------
# Milvus Lite iÃ§in dinamik kÃ¼tÃ¼phane yolu ayarÄ±
# Milvus Lite Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda libknowhere.so gibi paylaÅŸÄ±lan kÃ¼tÃ¼phaneleri
# bulamama hatalarÄ± yaÅŸanabilir. Bu durumun Ã¶nÃ¼ne geÃ§mek iÃ§in
# site-packages iÃ§indeki milvus_lite/lib klasÃ¶rÃ¼nÃ¼ LD_LIBRARY_PATH deÄŸiÅŸkenine
# ekliyoruz. Bu ayar, uygulama MacOS Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda gerek
# kalmayabilir ancak Linux tabanlÄ± ortamlar iÃ§in faydalÄ±dÄ±r.
lib_dir = None
for p in site.getsitepackages():
    candidate = os.path.join(p, "milvus_lite", "lib")
    if os.path.exists(candidate):
        lib_dir = candidate
        break
if lib_dir:
    current = os.environ.get("LD_LIBRARY_PATH", "")
    new_paths = lib_dir
    if current:
        new_paths = f"{lib_dir}:{current}"
    os.environ["LD_LIBRARY_PATH"] = new_paths

# -----------------------------------------------------------------------------
# Flask ve Milvus yapÄ±landÄ±rmasÄ±

app = Flask(__name__)

# Proje kÃ¶k dizini ve veritabanÄ± dosyasÄ±nÄ±n tanÄ±mlanmasÄ±
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "vector_db.db")

COLLECTION_NAME = "documents"
VECTOR_DIM = 768  # SentenceTransformer "paraphrase-multilingual-mpnet-base-v2" iÃ§in boyut

# Belgelerin vektÃ¶rleÅŸtirilmesi iÃ§in SentenceTransformer kullanÄ±yoruz.
vectorizer = SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")

# Milvus istemcisini baÅŸlatÄ±yoruz. DB_PATH parametresi yerel dosyaya iÅŸaret eder.
client = MilvusClient(DB_PATH)

# Koleksiyon mevcut deÄŸilse oluÅŸturulur. auto_id=True ayarÄ± ile birincil anahtar
# otomatik olarak oluÅŸturulur. VektÃ¶r alanÄ± FLOAT_VECTOR tÃ¼rÃ¼nde olmalÄ± ve
# boyutu Ã¶nceden belirlenmelidir.
if not client.has_collection(COLLECTION_NAME):
    schema = MilvusClient.create_schema(auto_id=True)
    schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True)
    schema.add_field(field_name="vector", datatype=DataType.FLOAT_VECTOR, dim=VECTOR_DIM)
    schema.add_field(field_name="text", datatype=DataType.VARCHAR, max_length=2000)
    schema.add_field(field_name="filename", datatype=DataType.VARCHAR, max_length=255)

    index_params = client.prepare_index_params()
    index_params.add_index(
        field_name="vector",
        index_type="AUTOINDEX",
        metric_type="COSINE",
    )
    client.create_collection(
        collection_name=COLLECTION_NAME,
        schema=schema,
        index_params=index_params,
    )

    # Ã–rnek veri dosyasÄ±ndan yÃ¼kleme
    example_file = os.path.join(BASE_DIR, "initial_data.txt")
    if os.path.exists(example_file):
        with open(example_file, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
            print(f"YÃ¼kleniyor: {len(lines)} satÄ±r initial_data.txt dosyasÄ±ndan.")
            for line in lines:
                print(f"VektÃ¶rleniyor ve ekleniyor: {line}")
            data = [
                {
                    "vector": vectorizer.encode(line).tolist(),
                    "text": line,
                    "filename": "initial_data.txt"
                }
                for line in lines
            ]
            client.insert(collection_name=COLLECTION_NAME, data=data)
            client.flush(collection_name=COLLECTION_NAME)


@app.route("/", methods=["GET"])
def index():
    """Ana sayfa: yÃ¼kleme ve arama formlarÄ±nÄ± gÃ¶sterir."""
    return render_template("index.html", results=None)


@app.route("/upload", methods=["POST"])
def upload():
    """Dosya veya metin yÃ¼kleme iÅŸlemlerini gerÃ§ekleÅŸtirir."""
    file = request.files.get("file")
    text_input = request.form.get("text_input")

    # YÃ¼klenecek metin ve dosya adÄ± deÄŸiÅŸkenleri
    text = None
    filename = None

    # Ã–ncelikle dosya yÃ¼klenmiÅŸse onu kullanÄ±yoruz
    if file and file.filename:
        # DosyayÄ± UTF-8 olarak Ã§Ã¶z; hata oluÅŸursa karakterleri yoksay
        content_bytes = file.read()
        try:
            text = content_bytes.decode("utf-8")
        except Exception:
            text = content_bytes.decode("utf-8", errors="ignore")
        filename = file.filename
    # Dosya yoksa metin kutusundaki veri kullanÄ±lÄ±r
    elif text_input:
        text = text_input
        filename = "ManualInput"

    # Metin yoksa ana sayfaya yÃ¶nlendir
    if not text:
        return redirect("/")

    # Metni vektÃ¶rleÅŸtirme
    vector = vectorizer.encode(text).tolist()

    # Milvus'a eklemek Ã¼zere veri nesnesi
    data = [
        {
            "vector": vector,
            "text": text,
            "filename": filename,
        }
    ]

    # Veriyi koleksiyona ekle
    client.insert(collection_name=COLLECTION_NAME, data=data)
    # Verinin baÅŸarÄ±yla yazÄ±ldÄ±ÄŸÄ±ndan emin olmak iÃ§in flush iÅŸlemi
    client.flush(collection_name=COLLECTION_NAME)

    return redirect("/")


@app.route("/search", methods=["POST"])
def search():
    """KullanÄ±cÄ±nÄ±n verdiÄŸi sorgu cÃ¼mlesiyle koleksiyon Ã¼zerinde arama yapar."""
    query = request.form.get("query")
    print(f"Arama sorgusu: {query}")
    if not query:
        return redirect("/")

    # Sorgu vektÃ¶rÃ¼nÃ¼ hazÄ±rla
    query_vector = vectorizer.encode(query).tolist()

    # Koleksiyonu belleÄŸe yÃ¼kle
    client.load_collection(collection_name=COLLECTION_NAME)

    # Arama iÅŸlemi: en benzer 10 sonucu getirir
    res = client.search(
        collection_name=COLLECTION_NAME,
        data=[query_vector],
        limit=10,
        output_fields=["text", "filename"],
        search_params={"metric_type": "COSINE"}
    )


    # SonuÃ§larÄ± liste haline getir, skoru normalize edip etiketle
    results = []
    for hits in res:
        for item in hits:
            raw = float(item.distance)
            # COSINE in newer PyMilvus typically returns similarity in [0,1] (higher is better).
            # Some setups may expose it as a distance (lower is better). Handle both robustly.
            if 0.0 <= raw <= 1.0:
                similarity = raw  # treat as cosine similarity
            else:
                similarity = max(0.0, min(1.0, 1.0 - raw))  # fallback: convert distance -> similarity
            score_percent = int(round(similarity * 100))

            print(
                f"Bulundu: {item.entity.get('text')} -> raw={raw:.4f} sim={similarity:.4f} percent={score_percent}"
            )

            # Initial label based on embedding similarity (will be adjusted after re-rank)
            if score_percent >= 85:
                label = "ğŸ”µ Ã‡ok benzer"
            elif score_percent >= 70:
                label = "ğŸŸ¢ Benzer"
            elif score_percent >= 60:
                label = "ğŸŸ¡ KÄ±smen benzer"
            else:
                label = "âšª DÃ¼ÅŸÃ¼k benzerlik"

            results.append(
                {
                    "filename": item.entity.get("filename"),
                    "text": item.entity.get("text"),
                    "score": f"{score_percent}%",
                    "score_num": score_percent,
                    "label": label,
                }
            )

    # --- Re-rank: combine embedding similarity with Turkish-aware keyword overlap ---
    q_tokens = norm_tokens(query or "")
    q_expanded = expand_query_terms(q_tokens)
    print(f"Re-rank: q_tokens={q_tokens} expanded={q_expanded}")

    for r in results:
        t_tokens = norm_tokens(r["text"])  # normalized + stemmed
        overlap_cnt = len(q_expanded & t_tokens)
        overlap_ratio = (overlap_cnt / len(q_expanded)) if q_expanded else 0.0
        overlap_pct = int(round(overlap_ratio * 100))
        r["overlap"] = overlap_pct

        # Blend: 60% embedding score, 40% keyword overlap (better for short queries)
        final_score = 0.6 * r.get("score_num", 0) + 0.4 * overlap_pct
        r["final_score"] = int(round(final_score))

        # Update primary score/label to reflect final ranking
        r["score_num"] = r["final_score"]
        r["score"] = f"{r['final_score']}%"
        if r["final_score"] >= 85:
            r["label"] = "ğŸ”µ Ã‡ok benzer"
        elif r["final_score"] >= 70:
            r["label"] = "ğŸŸ¢ Benzer"
        elif r["final_score"] >= 60:
            r["label"] = "ğŸŸ¡ KÄ±smen benzer"
        else:
            r["label"] = "âšª DÃ¼ÅŸÃ¼k benzerlik"

        print(f"Re-rank item: overlap={overlap_pct}% final={r['final_score']}% text={r['text'][:60]}...")

    print(f"Toplam bulunan sonuÃ§ sayÄ±sÄ± (ham): {len(results)}")
    # Uygulama eÅŸiÄŸi: 50. HiÃ§ sonuÃ§ kalmazsa, en iyi 5 sonucu eÅŸiÄŸe bakmadan gÃ¶ster.
    threshold = 50
    filtered_results = [r for r in results if r.get("final_score", 0) >= threshold]
    print(f"EÅŸik sonrasÄ± sonuÃ§ sayÄ±sÄ± (>={threshold}): {len(filtered_results)}")
    if not filtered_results:
        print("EÅŸik sonrasÄ± sonuÃ§ yok. En iyi 5 sonucu gÃ¶steriliyor (fallback).")
        filtered_results = sorted(results, key=lambda x: x.get("final_score", 0), reverse=True)[:5]
    results = sorted(filtered_results, key=lambda x: x.get("final_score", 0), reverse=True)

    return render_template('index.html', results=results, query=query)


if __name__ == "__main__":
    # Flask uygulamasÄ±nÄ± baÅŸlat
    # debug=True geliÅŸtirme ortamÄ±nda otomatik yeniden baÅŸlatma saÄŸlar.
    app.run(use_reloader=False)
